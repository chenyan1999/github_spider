{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6628e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def retrieve_file(repo_path: str, file_path: str, commit_sha: str):\n",
    "    '''\n",
    "    Func: Retrieve file of a given version\n",
    "    Args:\n",
    "        repo_path: str, the repository directory\n",
    "        file_path: str, the relative path for the file in \n",
    "                        terms of the repository\n",
    "        commit_sha: the commit version\n",
    "    '''\n",
    "    # Save the current working directory\n",
    "    original_dir = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        # Change to the repository directory\n",
    "        os.chdir(repo_path)\n",
    "\n",
    "        # Check out the specific commit\n",
    "        checkout_command = f'git checkout -q {commit_sha}'\n",
    "        subprocess.run(checkout_command, shell=True, check=True)\n",
    "\n",
    "        # Copy the file to the desired location\n",
    "        source_file = os.path.normpath(file_path)\n",
    "        with open(source_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        # Always return to the original working directory\n",
    "        os.chdir(original_dir)\n",
    "        return content\n",
    "    except:\n",
    "        # Always return to the original working directory\n",
    "        os.chdir(original_dir)\n",
    "        raise KeyError('Unable to find the file.')\n",
    "    \n",
    "def extract_info(old_file_path, new_file_path):\n",
    "    file_path = '/'.join(old_file_path.split('/')[3:])\n",
    "    elements = old_file_path.split('/')[2].split('_')\n",
    "    user_name = elements[0]\n",
    "    old_sha = elements[-1]\n",
    "    proj_name = '_'.join(elements[1:-1])\n",
    "    new_sha = new_file_path.split('/')[2].split('_')[-1]\n",
    "\n",
    "    return user_name, proj_name, old_sha, new_sha, file_path\n",
    "\n",
    "def contains_non_english_content(content):\n",
    "    if type(content) == list:\n",
    "        content = ''.join(content)\n",
    "    pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "    match = pattern.search(content)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def apply_filter(data_sample):\n",
    "    # 1. if code window is empty, return False\n",
    "    if len(data_sample['code_window']) == 0:\n",
    "        return False\n",
    "    # 2. if hunk type is 'add', but add_line is empty, return False\n",
    "    if data_sample['hunk_type'] == 'add' and len(data_sample['add_line']) == 0:\n",
    "        return False\n",
    "    # 3. if code_window, add_line, commit message contains more than english words, return False\n",
    "    if contains_non_english_content(data_sample['commit_msg']) or \\\n",
    "        contains_non_english_content(data_sample['add_line']) or \\\n",
    "        contains_non_english_content(data_sample['code_window']):\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13527d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7282\n",
      "Dataset example:\n",
      " {'user_name': 'Snailclimb', 'proj_name': 'JavaGuide', 'old_sha': 'b1576701bbbbfefa247e5d361e5b10f02e360e2a', 'new_sha': '789406cc28c9974334f2f07b0e85f0eb559d93d5', 'file_path': '分布式.md', 'changes': [{'func_name': '', 'del_line_idx': [], 'add_line_idx': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 'del_line': '', 'add_line': '  - ### 分布式系统的经典基础理论\\n  \\n    [分布式系统的经典基础理论](https://blog.csdn.net/qq_34337272)\\n  - ### 分布式事务\\n    [聊聊分布式事务，再说说解决方案](http://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html)\\n  - ### 分布式系统一致性\\n    [分布式服务化系统一致性的“最佳实干”](https://www.jianshu.com/p/1156151e20c8)\\n\\n   - ### 一致性协议/算法\\n     早在1898年就诞生了著名的 **Paxos经典算法** （**Zookeeper就采用了Paxos算法的近亲兄弟Zab算法**），但由于Paxos算法非常难以理解、实现、排错。所以不断有人尝试简化这一算法，直到2013年才有了重大突破：斯坦福的Diego Ongaro、John Ousterhout以易懂性为目标设计了新的一致性算法—— **Raft算法** ，并发布了对应的论文《In Search of an Understandable Consensus Algorithm》，到现在有十多种语言实现的Raft算法框架，较为出名的有以Go语言实现的Etcd，它的功能类似于Zookeeper，但采用了更为主流的Rest接口。\\n     * [图解 Paxos 一致性协议](http://blog.xiaohansong.com/2016/09/30/Paxos/)\\n     *  [图解分布式协议-RAFT](http://ifeve.com/raft/)\\n     *  [Zookeeper ZAB 协议分析](http://blog.xiaohansong.com/2016/08/25/zab/)'}], 'commit_msg': '增加分布式内容', 'pull_msg': '', 'html_url': 'https://github.com/Snailclimb/JavaGuide/commit/789406cc28c9974334f2f07b0e85f0eb559d93d5'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load raw dataset int jsonl format\n",
    "dataset = []\n",
    "with open('./dataset/java_dataset.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for i in file.readlines():\n",
    "        data = json.loads(i)\n",
    "        dataset.append(data)\n",
    "print('Dataset size:',len(dataset))\n",
    "print('Dataset example:\\n',dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c346bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of commits: 3709\n"
     ]
    }
   ],
   "source": [
    "# 2. categorize the dataset by commit_id\n",
    "# the unit of datasample of the raw dataset is a changed file, with multiple changed hunks\n",
    "# commit_id_dict: {commit_url: [the index of data samples in the raw dataset that belongs to this commit]}\n",
    "commit_id_dict = {}\n",
    "for idx, i in enumerate(dataset):\n",
    "    if i['html_url'] in commit_id_dict:\n",
    "        commit_id_dict[i['html_url']].append(idx)\n",
    "    else:\n",
    "        commit_id_dict[i['html_url']] = [idx]\n",
    "\n",
    "print('The number of commits:' ,len(commit_id_dict))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba28c85",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. convert each edit hunk into a data sample\n",
    "add = []\n",
    "replace = []\n",
    "remove = []\n",
    "prev_line = [3,4,5]\n",
    "repos_dir = './repos'\n",
    "processed_data_dir = ''\n",
    "\n",
    "for idx, i in enumerate(tqdm(dataset)):\n",
    "    cnt = 0\n",
    "    if 'old_file_path' in i.keys():\n",
    "        user_name, proj_name, old_sha, new_sha, file_path = extract_info(i['old_file_path'], i['new_file_path'])\n",
    "        i['user_name'] = user_name\n",
    "        i['proj_name'] = proj_name\n",
    "        i['old_sha'] = old_sha\n",
    "        i['new_sha'] = new_sha\n",
    "        i['file_path'] = file_path\n",
    "    for j in i['changes']:\n",
    "        # add\n",
    "        if j['del_line_idx'] == [] and j['add_line_idx']:\n",
    "            # with open('../'+i['new_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['new_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            # Get the content of this file of the given commit\n",
    "            try:\n",
    "                lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "            except KeyError:\n",
    "                break\n",
    "            except:\n",
    "                raise KeyError('Unexpected error:', sys.exc_info()[0])\n",
    "\n",
    "            label = ['keep']*len(lines)\n",
    "            \n",
    "            add_line = j['add_line_idx'][0]-1\n",
    "            start_line = j['add_line_idx'][0]-random.choice(prev_line)\n",
    "            mid_line = j['add_line_idx'][-1]\n",
    "            end_line = mid_line+random.choice(prev_line)+1\n",
    "            \n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "                \n",
    "            code_window = lines[start_line:add_line] + lines[mid_line:end_line]\n",
    "            label_window = label[start_line:add_line] + ['add'] + label[mid_line:end_line]\n",
    "            \n",
    "            data_sample = {\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'add'\n",
    "                    }\n",
    "            if apply_filter(data_sample):\n",
    "                add.append(data_sample)\n",
    "                \n",
    "        # replace\n",
    "        elif j['del_line_idx'] and j['add_line_idx']:\n",
    "            # with open('../'+i['old_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['old_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            try:\n",
    "                lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "            except KeyError:\n",
    "                break\n",
    "            except:\n",
    "                raise KeyError('Unexpected error:', sys.exc_info()[0])\n",
    "\n",
    "\n",
    "            label = ['keep']*len(lines)\n",
    "            for del_line in j['del_line_idx']:\n",
    "                label[del_line-1] = 'replace'\n",
    "            \n",
    "            start_line = j['del_line_idx'][0]-random.choice(prev_line)\n",
    "            end_line = j['del_line_idx'][-1]+random.choice(prev_line)\n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "            code_window = lines[start_line:end_line]\n",
    "            label_window = label[start_line:end_line]\n",
    "            \n",
    "            data_sample = {\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'replace'\n",
    "                    }\n",
    "            if apply_filter(data_sample):\n",
    "                replace.append(data_sample)\n",
    "        \n",
    "        # remove\n",
    "        elif j['del_line_idx'] and j['add_line_idx'] == []:\n",
    "            # with open('../'+i['old_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['old_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            try:\n",
    "                lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "            except KeyError:\n",
    "                break\n",
    "            except:\n",
    "                raise KeyError('Unexpected error:', sys.exc_info()[0])\n",
    "            \n",
    "            label = ['keep']*len(lines)\n",
    "            for del_line in j['del_line_idx']:\n",
    "                label[del_line-1] = 'remove'\n",
    "            \n",
    "            start_line = j['del_line_idx'][0]-random.choice(prev_line)\n",
    "            end_line = j['del_line_idx'][-1]+random.choice(prev_line)\n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "            code_window = lines[start_line:end_line]\n",
    "            label_window = label[start_line:end_line]\n",
    "            \n",
    "            data_sample = {\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'remove'\n",
    "                    }\n",
    "            if apply_filter(data_sample):\n",
    "                remove.append(data_sample)\n",
    "            \n",
    "print('The number of add type hunks:', len(add))\n",
    "print('The number of replace type hunks:', len(replace))\n",
    "print('The number of remove type hunks:', len(remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7a3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. categorize the data samples by commit_id\n",
    "# {commit url: [data samples in this commit]}\n",
    "result_dict = {} \n",
    "for i in add+replace+remove:\n",
    "    html_url = i['html_url']\n",
    "    if html_url in result_dict:\n",
    "        result_dict[html_url].append(i)\n",
    "    else:\n",
    "        result_dict[html_url] = [i]\n",
    "\n",
    "# save the dict into json file\n",
    "with open('./dataset/processed_dataset.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(result_dict, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779684b",
   "metadata": {},
   "source": [
    "# create dataset: edit generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bb54f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code window + label_window + commit message + prev_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "40ccbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank by similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "output = []\n",
    "for commit in sorted(result_dict.keys()):\n",
    "    for co_change in result_dict[commit]:\n",
    "        code_window = ''.join(co_change['code_window'])\n",
    "        label_window = ' '.join(co_change['label_window'])\n",
    "        commit_message = co_change['commit_msg']\n",
    "        context = []\n",
    "        if len(code_window) == 0:\n",
    "            continue\n",
    "        \n",
    "        # BM25 search for related context\n",
    "        prev_edit = result_dict[commit].copy()\n",
    "        prev_edit.remove(co_change)\n",
    "        try:\n",
    "            tokenized_corpus = [''.join(i['code_window']+[i['add_line']]).split() for i in prev_edit]\n",
    "            bm25 = BM25Okapi(tokenized_corpus) # build a BM25 object with other hunks\n",
    "            tokenized_query = code_window.split()\n",
    "            retrieval_code = bm25.get_top_n(tokenized_query, tokenized_corpus, n=5)\n",
    "            context_index = [tokenized_corpus.index(i) for i in retrieval_code] # get the index of the top 5 similar hunks\n",
    "\n",
    "            # form context, which are the deleted and added lines in the top 5 similar hunkss\n",
    "            for idx in context_index:\n",
    "                if prev_edit[idx]['hunk_type'] == 'replace': \n",
    "                    replace = prev_edit[idx]['label_window'].index('replace')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][replace])\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'remove':\n",
    "                    remove = prev_edit[idx]['label_window'].index('remove')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][remove])\n",
    "\n",
    "                elif prev_edit[idx]['label_window'] == 'add':\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        input_ = ' </s> '.join([code_window, label_window, commit_message] + context)\n",
    "        output_ =   co_change['add_line']\n",
    "        html_url =  co_change['html_url']\n",
    "        file_name = co_change['file_path']\n",
    "        output.append({\"docstring_tokens\":output_, \"code_tokens\":input_, \"html_url\":html_url, \"file_name\":file_name})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e0d085da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209585"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6efcf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "# final data format: {\"docstring_tokens\":doc_tokens, \"code_tokens\":code_tokens}\n",
    "os.path.join(processed_data_dir, 'generator/train.jsonl')\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/train.jsonl'), 'w') as f:\n",
    "    for item in output[:int(0.7*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/dev.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.7*len(output)): int(0.8*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/test.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.8*len(output)):]:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f747a2",
   "metadata": {},
   "source": [
    "# create dataset: edit locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank by similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "output = []\n",
    "for commit in result_dict:\n",
    "    for co_change in result_dict[commit]:\n",
    "        code_window = ''.join(co_change['code_window'])\n",
    "        label_window = ' '.join(co_change['label_window'])\n",
    "        commit_message = co_change['commit_msg']\n",
    "        context = []\n",
    "        if len(code_window) == 0:\n",
    "            continue\n",
    "        \n",
    "        # BM25 search for related context\n",
    "        prev_edit = result_dict[commit].copy()\n",
    "        prev_edit.remove(co_change)\n",
    "        try:\n",
    "            tokenized_corpus = [''.join(i['code_window']+[i['add_line']]).split() for i in prev_edit]\n",
    "            bm25 = BM25Okapi(tokenized_corpus)\n",
    "            tokenized_query = code_window.split()\n",
    "            retrieval_code = bm25.get_top_n(tokenized_query, tokenized_corpus, n=5)\n",
    "            context_index = [tokenized_corpus.index(i) for i in retrieval_code]\n",
    "\n",
    "            for idx in context_index:\n",
    "                if prev_edit[idx]['hunk_type'] == 'replace':\n",
    "                    replace = prev_edit[idx]['label_window'].index('replace')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][replace])\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'remove':\n",
    "                    remove = prev_edit[idx]['label_window'].index('remove')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][remove])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'add':\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        input_ = ' </s> '.join([code_window, commit_message] + context)\n",
    "        output_ =   co_change['add_line']\n",
    "        html_url =  co_change['html_url']\n",
    "        file_name = co_change['file_path']\n",
    "        output.append({\"docstring_tokens\":label_window, \"code_tokens\":input_, \"html_url\":html_url, \"file_name\":file_name})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33545a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "# create dataset: code generation\n",
    "# final data format: {\"docstring_tokens\":doc_tokens, \"code_tokens\":code_tokens}\n",
    "\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/train.jsonl'), 'w') as f:\n",
    "    for item in output[:int(0.7*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/dev.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.7*len(output)): int(0.8*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/test.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.8*len(output)):]:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988f5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
