{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "def retrieve_file(repo_path: str, file_path: str, commit_sha: str):\n",
    "    '''\n",
    "    Func: Retrieve file of a given version\n",
    "    Args:\n",
    "        repo_path: str, the repository directory\n",
    "        file_path: str, the relative path for the file in \n",
    "                        terms of the repository\n",
    "        commit_sha: the commit version\n",
    "    '''\n",
    "    # Save the current working directory\n",
    "    original_dir = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        # Change to the repository directory\n",
    "        os.chdir(repo_path)\n",
    "\n",
    "        # Check out the specific commit\n",
    "        checkout_command = f'git checkout {commit_sha}'\n",
    "        subprocess.run(checkout_command, shell=True, check=True)\n",
    "\n",
    "        # Copy the file to the desired location\n",
    "        source_file = os.path.normpath(file_path)\n",
    "        with open(source_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.readlines()\n",
    "\n",
    "        # Always return to the original working directory\n",
    "        os.chdir(original_dir)\n",
    "        return content\n",
    "    except:\n",
    "        # Always return to the original working directory\n",
    "        os.chdir(original_dir)\n",
    "        raise KeyError('Unable to find the file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13527d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load raw dataset int jsonl format\n",
    "dataset = []\n",
    "with open('javascript_dataset.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for i in file.readlines():\n",
    "        data = json.loads(i)\n",
    "        dataset.append(data)\n",
    "print('Dataset size:',len(dataset))\n",
    "print('Dataset example:',dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c346bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19951"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. categorize the dataset by commit_id\n",
    "# the unit of datasample of the raw dataset is a changed file, with multiple changed hunks\n",
    "# commit_id_dict: {commit_url: [the index of data samples in the raw dataset that belongs to this commit]}\n",
    "commit_id_dict = {}\n",
    "for idx, i in enumerate(dataset):\n",
    "    if i['html_url'] in commit_id_dict:\n",
    "        commit_id_dict[i['html_url']].append(idx)\n",
    "    else:\n",
    "        commit_id_dict[i['html_url']] = [idx]\n",
    "\n",
    "print('The number of commits:' ,len(commit_id_dict))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba28c85",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ffdeec",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.././repos/alekseykulikov_storage_0065c63f6ffa6af38fcdcebf4f3a1af21ce7af4e/component.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h6/2scw0c215yn2f_z5rclvgvrc0000gn/T/ipykernel_10837/3369635426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'del_line_idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'add_line_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_file_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'keep'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.././repos/alekseykulikov_storage_0065c63f6ffa6af38fcdcebf4f3a1af21ce7af4e/component.json'"
     ]
    }
   ],
   "source": [
    "# 3. convert each edit hunk into a data sample\n",
    "add = []\n",
    "replace = []\n",
    "remove = []\n",
    "prev_line = [3,4,5]\n",
    "repos_dir = '/media/chenyan/Backup Plus/CodeEdit_raw_dataset/repos'\n",
    "processed_data_dir = ''\n",
    "\n",
    "for idx, i in enumerate(dataset):\n",
    "    cnt = 0\n",
    "    for j in i['changes']:\n",
    "        # add\n",
    "        if j['del_line_idx'] == [] and j['add_line_idx']:\n",
    "            # with open('../'+i['new_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['new_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            # Get the content of this file of the given commit\n",
    "            lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "\n",
    "            label = ['keep']*len(lines)\n",
    "            \n",
    "            add_line = j['add_line_idx'][0]-1\n",
    "            start_line = j['add_line_idx'][0]-random.choice(prev_line)\n",
    "            mid_line = j['add_line_idx'][-1]\n",
    "            end_line = mid_line+random.choice(prev_line)+1\n",
    "            \n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "                \n",
    "            code_window = lines[start_line:add_line] + lines[mid_line:end_line]\n",
    "            label_window = label[start_line:add_line] + ['add'] + label[mid_line:end_line]\n",
    "            \n",
    "            add.append({\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'add'\n",
    "                    })\n",
    "                \n",
    "        # replace\n",
    "        elif j['del_line_idx'] and j['add_line_idx']:\n",
    "            # with open('../'+i['old_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['old_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "\n",
    "            label = ['keep']*len(lines)\n",
    "            for del_line in j['del_line_idx']:\n",
    "                label[del_line-1] = 'replace'\n",
    "            \n",
    "            start_line = j['del_line_idx'][0]-random.choice(prev_line)\n",
    "            end_line = j['del_line_idx'][-1]+random.choice(prev_line)\n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "            code_window = lines[start_line:end_line]\n",
    "            label_window = label[start_line:end_line]\n",
    "            \n",
    "            replace.append({\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'replace'\n",
    "                    })\n",
    "        \n",
    "        # remove\n",
    "        elif j['del_line_idx'] and j['add_line_idx'] == []:\n",
    "            # with open('../'+i['old_file_path'], encoding='utf-8') as file:\n",
    "            proj_name = i['proj_name']\n",
    "            commit_sha = i['old_sha']\n",
    "            file_path = i['file_path']\n",
    "            repo_path = os.path.join(repos_dir, proj_name)\n",
    "            lines = retrieve_file(repo_path, file_path, commit_sha)\n",
    "            \n",
    "            label = ['keep']*len(lines)\n",
    "            for del_line in j['del_line_idx']:\n",
    "                label[del_line-1] = 'remove'\n",
    "            \n",
    "            start_line = j['del_line_idx'][0]-random.choice(prev_line)\n",
    "            end_line = j['del_line_idx'][-1]+random.choice(prev_line)\n",
    "            if start_line < 0:\n",
    "                start_line = 0\n",
    "            if end_line > len(lines):\n",
    "                end_line = -1\n",
    "            code_window = lines[start_line:end_line]\n",
    "            label_window = label[start_line:end_line]\n",
    "            \n",
    "            remove.append({\n",
    "                        'code_window': code_window, \n",
    "                        'label_window': label_window, \n",
    "                        'commit_msg': i['commit_msg'], \n",
    "                        'html_url': i['html_url'], \n",
    "                        'add_line': j['add_line'], \n",
    "                        'method_name': j['func_name'],\n",
    "                        'old_file_path': file_path, \n",
    "                        'idx': idx,\n",
    "                        'hunk_type': 'remove'\n",
    "                    })\n",
    "            \n",
    "print('The number of add type hunks:', len(add))\n",
    "print('The number of replace type hunks:', len(replace))\n",
    "print('The number of remove type hunks:', len(remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4a7a3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. categorize the data samples by commit_id\n",
    "# {commit url: [data samples in this commit]}\n",
    "result_dict = {} \n",
    "for i in add+replace+remove:\n",
    "    html_url = i['html_url']\n",
    "    if html_url in result_dict:\n",
    "        result_dict[html_url].append(i)\n",
    "    else:\n",
    "        result_dict[html_url] = [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779684b",
   "metadata": {},
   "source": [
    "# create dataset: edit generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bb54f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code window + label_window + commit message + prev_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "40ccbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank by similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "output = []\n",
    "for commit in sorted(result_dict.keys()):\n",
    "    for co_change in result_dict[commit]:\n",
    "        code_window = ''.join(co_change['code_window'])\n",
    "        label_window = ' '.join(co_change['label_window'])\n",
    "        commit_message = co_change['commit_msg']\n",
    "        context = []\n",
    "        if len(code_window) == 0:\n",
    "            continue\n",
    "        \n",
    "        # BM25 search for related context\n",
    "        prev_edit = result_dict[commit].copy()\n",
    "        prev_edit.remove(co_change)\n",
    "        try:\n",
    "            tokenized_corpus = [''.join(i['code_window']+[i['add_line']]).split() for i in prev_edit]\n",
    "            bm25 = BM25Okapi(tokenized_corpus) # build a BM25 object with other hunks\n",
    "            tokenized_query = code_window.split()\n",
    "            retrieval_code = bm25.get_top_n(tokenized_query, tokenized_corpus, n=5)\n",
    "            context_index = [tokenized_corpus.index(i) for i in retrieval_code] # get the index of the top 5 similar hunks\n",
    "\n",
    "            # form context, which are the deleted and added lines in the top 5 similar hunkss\n",
    "            for idx in context_index:\n",
    "                if prev_edit[idx]['hunk_type'] == 'replace': \n",
    "                    replace = prev_edit[idx]['label_window'].index('replace')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][replace])\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'remove':\n",
    "                    remove = prev_edit[idx]['label_window'].index('remove')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][remove])\n",
    "\n",
    "                elif prev_edit[idx]['label_window'] == 'add':\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        input_ = ' </s> '.join([code_window, label_window, commit_message] + context)\n",
    "        output_ =   co_change['add_line']\n",
    "        html_url =  co_change['html_url']\n",
    "        file_name = co_change['file_path']\n",
    "        output.append({\"docstring_tokens\":output_, \"code_tokens\":input_, \"html_url\":html_url, \"file_name\":file_name})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e0d085da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209585"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6efcf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "# final data format: {\"docstring_tokens\":doc_tokens, \"code_tokens\":code_tokens}\n",
    "os.path.join(processed_data_dir, 'generator/train.jsonl')\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/train.jsonl'), 'w') as f:\n",
    "    for item in output[:int(0.7*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/dev.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.7*len(output)): int(0.8*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'generator/test.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.8*len(output)):]:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f747a2",
   "metadata": {},
   "source": [
    "# create dataset: edit locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank by similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "output = []\n",
    "for commit in result_dict:\n",
    "    for co_change in result_dict[commit]:\n",
    "        code_window = ''.join(co_change['code_window'])\n",
    "        label_window = ' '.join(co_change['label_window'])\n",
    "        commit_message = co_change['commit_msg']\n",
    "        context = []\n",
    "        if len(code_window) == 0:\n",
    "            continue\n",
    "        \n",
    "        # BM25 search for related context\n",
    "        prev_edit = result_dict[commit].copy()\n",
    "        prev_edit.remove(co_change)\n",
    "        try:\n",
    "            tokenized_corpus = [''.join(i['code_window']+[i['add_line']]).split() for i in prev_edit]\n",
    "            bm25 = BM25Okapi(tokenized_corpus)\n",
    "            tokenized_query = code_window.split()\n",
    "            retrieval_code = bm25.get_top_n(tokenized_query, tokenized_corpus, n=5)\n",
    "            context_index = [tokenized_corpus.index(i) for i in retrieval_code]\n",
    "\n",
    "            for idx in context_index:\n",
    "                if prev_edit[idx]['hunk_type'] == 'replace':\n",
    "                    replace = prev_edit[idx]['label_window'].index('replace')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][replace])\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'remove':\n",
    "                    remove = prev_edit[idx]['label_window'].index('remove')\n",
    "                    context.append('remove '+ prev_edit[idx]['code_window'][remove])\n",
    "\n",
    "                elif prev_edit[idx]['hunk_type'] == 'add':\n",
    "                    context.append('add '+ prev_edit[idx]['add_line'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        input_ = ' </s> '.join([code_window, commit_message] + context)\n",
    "        output_ =   co_change['add_line']\n",
    "        html_url =  co_change['html_url']\n",
    "        file_name = co_change['file_path']\n",
    "        output.append({\"docstring_tokens\":label_window, \"code_tokens\":input_, \"html_url\":html_url, \"file_name\":file_name})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33545a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "# create dataset: code generation\n",
    "# final data format: {\"docstring_tokens\":doc_tokens, \"code_tokens\":code_tokens}\n",
    "\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/train.jsonl'), 'w') as f:\n",
    "    for item in output[:int(0.7*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/dev.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.7*len(output)): int(0.8*len(output))]:\n",
    "        f.write(item)\n",
    "with jsonlines.open(os.path.join(processed_data_dir, 'locator/test.jsonl'), 'w') as f:\n",
    "    for item in output[int(0.8*len(output)):]:\n",
    "        f.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988f5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
